{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSjYkN2Uy9Sz",
        "outputId": "93568bc1-c98c-4d80-ffbc-5a1359b309c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting up data preparation...\n",
            "Downloading Fashion-MNIST dataset...\n",
            "Defining lightweight model...\n",
            "Starting model training...\n",
            "Training on device: cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Batch 100: Loss: 1.624, Accuracy: 47.50%\n",
            "Epoch 1, Batch 200: Loss: 1.398, Accuracy: 50.25%\n",
            "Epoch 1, Batch 300: Loss: 1.204, Accuracy: 54.08%\n",
            "Epoch 1, Batch 400: Loss: 0.974, Accuracy: 57.75%\n",
            "Epoch 1, Batch 500: Loss: 0.954, Accuracy: 60.00%\n",
            "Epoch 1, Batch 600: Loss: 1.061, Accuracy: 61.08%\n",
            "Epoch 1, Batch 700: Loss: 0.909, Accuracy: 62.21%\n",
            "Epoch 1, Batch 800: Loss: 0.877, Accuracy: 63.56%\n",
            "Epoch 1, Batch 900: Loss: 0.862, Accuracy: 64.47%\n",
            "Epoch 1, Batch 1000: Loss: 0.923, Accuracy: 65.22%\n",
            "Epoch 1, Batch 1100: Loss: 0.880, Accuracy: 65.93%\n",
            "Epoch 1, Batch 1200: Loss: 0.783, Accuracy: 66.56%\n",
            "Epoch 1, Batch 1300: Loss: 0.773, Accuracy: 67.02%\n",
            "Epoch 1, Batch 1400: Loss: 0.820, Accuracy: 67.48%\n",
            "Epoch 1, Batch 1500: Loss: 0.791, Accuracy: 68.18%\n",
            "Epoch 1, Batch 1600: Loss: 0.621, Accuracy: 69.03%\n",
            "Epoch 1, Batch 1700: Loss: 0.711, Accuracy: 69.56%\n",
            "Epoch 1, Batch 1800: Loss: 0.766, Accuracy: 69.97%\n",
            "Epoch 1, Batch 1900: Loss: 0.626, Accuracy: 70.57%\n",
            "Epoch 1, Batch 2000: Loss: 0.636, Accuracy: 71.24%\n",
            "Epoch 1, Batch 2100: Loss: 0.814, Accuracy: 71.57%\n",
            "Epoch 1, Batch 2200: Loss: 0.805, Accuracy: 71.90%\n",
            "Epoch 1, Batch 2300: Loss: 0.657, Accuracy: 72.17%\n",
            "Epoch 1, Batch 2400: Loss: 0.772, Accuracy: 72.35%\n",
            "Epoch 1, Batch 2500: Loss: 0.614, Accuracy: 72.71%\n",
            "Epoch 1, Batch 2600: Loss: 0.739, Accuracy: 72.99%\n",
            "Epoch 1, Batch 2700: Loss: 0.728, Accuracy: 73.23%\n",
            "Epoch 1, Batch 2800: Loss: 0.765, Accuracy: 73.43%\n",
            "Epoch 1, Batch 2900: Loss: 0.648, Accuracy: 73.72%\n",
            "Epoch 1, Batch 3000: Loss: 0.732, Accuracy: 73.91%\n",
            "Epoch 1, Batch 3100: Loss: 0.681, Accuracy: 74.11%\n",
            "Epoch 1, Batch 3200: Loss: 0.564, Accuracy: 74.41%\n",
            "Epoch 1, Batch 3300: Loss: 0.544, Accuracy: 74.68%\n",
            "Epoch 1, Batch 3400: Loss: 0.734, Accuracy: 74.79%\n",
            "Epoch 1, Batch 3500: Loss: 0.626, Accuracy: 74.99%\n",
            "Epoch 1, Batch 3600: Loss: 0.646, Accuracy: 75.19%\n",
            "Epoch 1, Batch 3700: Loss: 0.630, Accuracy: 75.34%\n",
            "Epoch 1, Batch 3800: Loss: 0.595, Accuracy: 75.57%\n",
            "Epoch 1, Batch 3900: Loss: 0.632, Accuracy: 75.76%\n",
            "Epoch 1, Batch 4000: Loss: 0.647, Accuracy: 75.88%\n",
            "Epoch 1, Batch 4100: Loss: 0.428, Accuracy: 76.15%\n",
            "Epoch 1, Batch 4200: Loss: 0.618, Accuracy: 76.28%\n",
            "Epoch 1, Batch 4300: Loss: 0.694, Accuracy: 76.37%\n",
            "Epoch 1, Batch 4400: Loss: 0.502, Accuracy: 76.55%\n",
            "Epoch 1, Batch 4500: Loss: 0.564, Accuracy: 76.74%\n",
            "Epoch 1, Batch 4600: Loss: 0.462, Accuracy: 76.93%\n",
            "Epoch 1, Batch 4700: Loss: 0.685, Accuracy: 77.05%\n",
            "Epoch 1, Batch 4800: Loss: 0.529, Accuracy: 77.24%\n",
            "Epoch 1, Batch 4900: Loss: 0.619, Accuracy: 77.38%\n",
            "Epoch 1, Batch 5000: Loss: 0.535, Accuracy: 77.53%\n",
            "Epoch 1, Batch 5100: Loss: 0.575, Accuracy: 77.71%\n",
            "Epoch 1, Batch 5200: Loss: 0.574, Accuracy: 77.79%\n",
            "Epoch 1, Batch 5300: Loss: 0.550, Accuracy: 77.91%\n",
            "Epoch 1, Batch 5400: Loss: 0.499, Accuracy: 78.06%\n",
            "Epoch 1, Batch 5500: Loss: 0.581, Accuracy: 78.19%\n",
            "Epoch 1, Batch 5600: Loss: 0.631, Accuracy: 78.33%\n",
            "Epoch 1, Batch 5700: Loss: 0.491, Accuracy: 78.48%\n",
            "Epoch 1, Batch 5800: Loss: 0.603, Accuracy: 78.53%\n",
            "Epoch 1, Batch 5900: Loss: 0.554, Accuracy: 78.65%\n",
            "Epoch 1, Batch 6000: Loss: 0.479, Accuracy: 78.77%\n",
            "Epoch 1, Batch 6100: Loss: 0.345, Accuracy: 78.93%\n",
            "Epoch 1, Batch 6200: Loss: 0.481, Accuracy: 79.08%\n",
            "Epoch 1, Batch 6300: Loss: 0.415, Accuracy: 79.23%\n",
            "Epoch 1, Batch 6400: Loss: 0.564, Accuracy: 79.33%\n",
            "Epoch 1, Batch 6500: Loss: 0.493, Accuracy: 79.41%\n",
            "Epoch 1, Batch 6600: Loss: 0.486, Accuracy: 79.48%\n",
            "Epoch 1, Batch 6700: Loss: 0.571, Accuracy: 79.56%\n",
            "Epoch 1, Batch 6800: Loss: 0.443, Accuracy: 79.65%\n",
            "Epoch 1, Batch 6900: Loss: 0.582, Accuracy: 79.71%\n",
            "Epoch 1, Batch 7000: Loss: 0.515, Accuracy: 79.76%\n",
            "Epoch 1, Batch 7100: Loss: 0.465, Accuracy: 79.90%\n",
            "Epoch 1, Batch 7200: Loss: 0.362, Accuracy: 80.04%\n",
            "Epoch 1, Batch 7300: Loss: 0.531, Accuracy: 80.11%\n",
            "Epoch 1, Batch 7400: Loss: 0.648, Accuracy: 80.15%\n",
            "Epoch 1, Batch 7500: Loss: 0.516, Accuracy: 80.21%\n",
            "Epoch 1, Batch 7600: Loss: 0.381, Accuracy: 80.31%\n",
            "Epoch 1, Batch 7700: Loss: 0.413, Accuracy: 80.45%\n",
            "Epoch 1, Batch 7800: Loss: 0.502, Accuracy: 80.54%\n",
            "Epoch 1, Batch 7900: Loss: 0.587, Accuracy: 80.59%\n",
            "Epoch 1, Batch 8000: Loss: 0.391, Accuracy: 80.70%\n",
            "Epoch 1, Batch 8100: Loss: 0.474, Accuracy: 80.77%\n",
            "Epoch 1, Batch 8200: Loss: 0.342, Accuracy: 80.87%\n",
            "Epoch 1, Batch 8300: Loss: 0.508, Accuracy: 80.94%\n",
            "Epoch 1, Batch 8400: Loss: 0.486, Accuracy: 81.03%\n",
            "Epoch 1, Batch 8500: Loss: 0.525, Accuracy: 81.07%\n",
            "Epoch 1, Batch 8600: Loss: 0.514, Accuracy: 81.12%\n",
            "Epoch 1, Batch 8700: Loss: 0.441, Accuracy: 81.20%\n",
            "Epoch 1, Batch 8800: Loss: 0.469, Accuracy: 81.27%\n",
            "Epoch 1, Batch 8900: Loss: 0.537, Accuracy: 81.34%\n",
            "Epoch 1, Batch 9000: Loss: 0.481, Accuracy: 81.39%\n",
            "Epoch 1, Batch 9100: Loss: 0.562, Accuracy: 81.42%\n",
            "Epoch 1, Batch 9200: Loss: 0.478, Accuracy: 81.46%\n",
            "Epoch 1, Batch 9300: Loss: 0.417, Accuracy: 81.54%\n",
            "Epoch 1, Batch 9400: Loss: 0.441, Accuracy: 81.60%\n",
            "Epoch 1, Batch 9500: Loss: 0.578, Accuracy: 81.62%\n",
            "Epoch 1, Batch 9600: Loss: 0.377, Accuracy: 81.72%\n",
            "Epoch 1, Batch 9700: Loss: 0.521, Accuracy: 81.77%\n",
            "Epoch 1, Batch 9800: Loss: 0.376, Accuracy: 81.85%\n",
            "Epoch 1, Batch 9900: Loss: 0.535, Accuracy: 81.89%\n",
            "Epoch 1, Batch 10000: Loss: 0.416, Accuracy: 81.97%\n",
            "Epoch 1, Batch 10100: Loss: 0.419, Accuracy: 82.04%\n",
            "Epoch 1, Batch 10200: Loss: 0.580, Accuracy: 82.06%\n",
            "Epoch 1, Batch 10300: Loss: 0.403, Accuracy: 82.12%\n",
            "Epoch 1, Batch 10400: Loss: 0.503, Accuracy: 82.18%\n",
            "Epoch 1, Batch 10500: Loss: 0.496, Accuracy: 82.22%\n",
            "Epoch 1, Batch 10600: Loss: 0.388, Accuracy: 82.26%\n",
            "Epoch 1, Batch 10700: Loss: 0.489, Accuracy: 82.31%\n",
            "Epoch 1, Batch 10800: Loss: 0.377, Accuracy: 82.37%\n",
            "Epoch 1, Batch 10900: Loss: 0.448, Accuracy: 82.43%\n",
            "Epoch 1, Batch 11000: Loss: 0.369, Accuracy: 82.51%\n",
            "Epoch 1, Batch 11100: Loss: 0.449, Accuracy: 82.56%\n",
            "Epoch 1, Batch 11200: Loss: 0.429, Accuracy: 82.60%\n",
            "Epoch 1, Batch 11300: Loss: 0.416, Accuracy: 82.65%\n",
            "Epoch 1, Batch 11400: Loss: 0.525, Accuracy: 82.69%\n",
            "Epoch 1, Batch 11500: Loss: 0.506, Accuracy: 82.73%\n",
            "Epoch 1, Batch 11600: Loss: 0.407, Accuracy: 82.78%\n",
            "Epoch 1, Batch 11700: Loss: 0.456, Accuracy: 82.82%\n",
            "Epoch 1, Batch 11800: Loss: 0.423, Accuracy: 82.85%\n",
            "Epoch 1, Batch 11900: Loss: 0.474, Accuracy: 82.87%\n",
            "Epoch 1, Batch 12000: Loss: 0.472, Accuracy: 82.91%\n",
            "Epoch 1, Batch 12100: Loss: 0.358, Accuracy: 82.99%\n",
            "Epoch 1, Batch 12200: Loss: 0.478, Accuracy: 83.04%\n",
            "Epoch 1, Batch 12300: Loss: 0.505, Accuracy: 83.07%\n",
            "Epoch 1, Batch 12400: Loss: 0.450, Accuracy: 83.10%\n",
            "Epoch 1, Batch 12500: Loss: 0.481, Accuracy: 83.11%\n",
            "Epoch 1, Batch 12600: Loss: 0.405, Accuracy: 83.14%\n",
            "Epoch 1, Batch 12700: Loss: 0.486, Accuracy: 83.17%\n",
            "Epoch 1, Batch 12800: Loss: 0.382, Accuracy: 83.21%\n",
            "Epoch 1, Batch 12900: Loss: 0.432, Accuracy: 83.26%\n",
            "Epoch 1, Batch 13000: Loss: 0.377, Accuracy: 83.29%\n",
            "Epoch 1, Batch 13100: Loss: 0.517, Accuracy: 83.33%\n",
            "Epoch 1, Batch 13200: Loss: 0.389, Accuracy: 83.37%\n",
            "Epoch 1, Batch 13300: Loss: 0.392, Accuracy: 83.42%\n",
            "Epoch 1, Batch 13400: Loss: 0.384, Accuracy: 83.47%\n",
            "Epoch 1, Batch 13500: Loss: 0.480, Accuracy: 83.49%\n",
            "Epoch 1, Batch 13600: Loss: 0.537, Accuracy: 83.51%\n",
            "Epoch 1, Batch 13700: Loss: 0.475, Accuracy: 83.53%\n",
            "Epoch 1, Batch 13800: Loss: 0.372, Accuracy: 83.57%\n",
            "Epoch 1, Batch 13900: Loss: 0.380, Accuracy: 83.61%\n",
            "Epoch 1, Batch 14000: Loss: 0.553, Accuracy: 83.63%\n",
            "Epoch 1, Batch 14100: Loss: 0.452, Accuracy: 83.65%\n",
            "Epoch 1, Batch 14200: Loss: 0.329, Accuracy: 83.70%\n",
            "Epoch 1, Batch 14300: Loss: 0.561, Accuracy: 83.72%\n",
            "Epoch 1, Batch 14400: Loss: 0.324, Accuracy: 83.76%\n",
            "Epoch 1, Batch 14500: Loss: 0.371, Accuracy: 83.80%\n",
            "Epoch 1, Batch 14600: Loss: 0.322, Accuracy: 83.85%\n",
            "Epoch 1, Batch 14700: Loss: 0.440, Accuracy: 83.87%\n",
            "Epoch 1, Batch 14800: Loss: 0.317, Accuracy: 83.92%\n",
            "Epoch 1, Batch 14900: Loss: 0.437, Accuracy: 83.95%\n",
            "Epoch 1, Batch 15000: Loss: 0.438, Accuracy: 83.99%\n",
            "Validation Accuracy after Epoch 1: 88.84%\n",
            "Epoch 2, Batch 100: Loss: 0.312, Accuracy: 90.50%\n",
            "Epoch 2, Batch 200: Loss: 0.525, Accuracy: 88.75%\n",
            "Epoch 2, Batch 300: Loss: 0.438, Accuracy: 88.58%\n",
            "Epoch 2, Batch 400: Loss: 0.301, Accuracy: 89.38%\n",
            "Epoch 2, Batch 500: Loss: 0.331, Accuracy: 89.65%\n",
            "Epoch 2, Batch 600: Loss: 0.332, Accuracy: 89.58%\n",
            "Epoch 2, Batch 700: Loss: 0.376, Accuracy: 89.54%\n",
            "Epoch 2, Batch 800: Loss: 0.362, Accuracy: 89.41%\n",
            "Epoch 2, Batch 900: Loss: 0.234, Accuracy: 89.67%\n",
            "Epoch 2, Batch 1000: Loss: 0.400, Accuracy: 89.70%\n",
            "Epoch 2, Batch 1100: Loss: 0.297, Accuracy: 89.84%\n",
            "Epoch 2, Batch 1200: Loss: 0.233, Accuracy: 90.21%\n",
            "Epoch 2, Batch 1300: Loss: 0.239, Accuracy: 90.33%\n",
            "Epoch 2, Batch 1400: Loss: 0.355, Accuracy: 90.18%\n",
            "Epoch 2, Batch 1500: Loss: 0.339, Accuracy: 90.17%\n",
            "Epoch 2, Batch 1600: Loss: 0.461, Accuracy: 90.05%\n",
            "Epoch 2, Batch 1700: Loss: 0.379, Accuracy: 90.10%\n",
            "Epoch 2, Batch 1800: Loss: 0.301, Accuracy: 90.07%\n",
            "Epoch 2, Batch 1900: Loss: 0.400, Accuracy: 90.05%\n",
            "Epoch 2, Batch 2000: Loss: 0.330, Accuracy: 90.10%\n",
            "Epoch 2, Batch 2100: Loss: 0.296, Accuracy: 90.08%\n",
            "Epoch 2, Batch 2200: Loss: 0.373, Accuracy: 90.11%\n",
            "Epoch 2, Batch 2300: Loss: 0.298, Accuracy: 90.12%\n",
            "Epoch 2, Batch 2400: Loss: 0.358, Accuracy: 90.07%\n",
            "Epoch 2, Batch 2500: Loss: 0.311, Accuracy: 90.04%\n",
            "Epoch 2, Batch 2600: Loss: 0.264, Accuracy: 90.10%\n",
            "Epoch 2, Batch 2700: Loss: 0.327, Accuracy: 90.15%\n",
            "Epoch 2, Batch 2800: Loss: 0.301, Accuracy: 90.21%\n",
            "Epoch 2, Batch 2900: Loss: 0.329, Accuracy: 90.22%\n",
            "Epoch 2, Batch 3000: Loss: 0.350, Accuracy: 90.19%\n",
            "Epoch 2, Batch 3100: Loss: 0.405, Accuracy: 90.21%\n",
            "Epoch 2, Batch 3200: Loss: 0.375, Accuracy: 90.19%\n",
            "Epoch 2, Batch 3300: Loss: 0.329, Accuracy: 90.17%\n",
            "Epoch 2, Batch 3400: Loss: 0.313, Accuracy: 90.21%\n",
            "Epoch 2, Batch 3500: Loss: 0.338, Accuracy: 90.22%\n",
            "Epoch 2, Batch 3600: Loss: 0.380, Accuracy: 90.22%\n",
            "Epoch 2, Batch 3700: Loss: 0.397, Accuracy: 90.21%\n",
            "Epoch 2, Batch 3800: Loss: 0.314, Accuracy: 90.22%\n",
            "Epoch 2, Batch 3900: Loss: 0.365, Accuracy: 90.21%\n",
            "Epoch 2, Batch 4000: Loss: 0.305, Accuracy: 90.26%\n",
            "Epoch 2, Batch 4100: Loss: 0.325, Accuracy: 90.27%\n",
            "Epoch 2, Batch 4200: Loss: 0.317, Accuracy: 90.26%\n",
            "Epoch 2, Batch 4300: Loss: 0.331, Accuracy: 90.24%\n",
            "Epoch 2, Batch 4400: Loss: 0.351, Accuracy: 90.27%\n",
            "Epoch 2, Batch 4500: Loss: 0.442, Accuracy: 90.25%\n",
            "Epoch 2, Batch 4600: Loss: 0.263, Accuracy: 90.28%\n",
            "Epoch 2, Batch 4700: Loss: 0.334, Accuracy: 90.30%\n",
            "Epoch 2, Batch 4800: Loss: 0.318, Accuracy: 90.31%\n",
            "Epoch 2, Batch 4900: Loss: 0.420, Accuracy: 90.24%\n",
            "Epoch 2, Batch 5000: Loss: 0.292, Accuracy: 90.26%\n",
            "Epoch 2, Batch 5100: Loss: 0.310, Accuracy: 90.28%\n",
            "Epoch 2, Batch 5200: Loss: 0.255, Accuracy: 90.32%\n",
            "Epoch 2, Batch 5300: Loss: 0.253, Accuracy: 90.39%\n",
            "Epoch 2, Batch 5400: Loss: 0.390, Accuracy: 90.38%\n",
            "Epoch 2, Batch 5500: Loss: 0.412, Accuracy: 90.35%\n",
            "Epoch 2, Batch 5600: Loss: 0.368, Accuracy: 90.37%\n",
            "Epoch 2, Batch 5700: Loss: 0.393, Accuracy: 90.36%\n",
            "Epoch 2, Batch 5800: Loss: 0.348, Accuracy: 90.37%\n",
            "Epoch 2, Batch 5900: Loss: 0.384, Accuracy: 90.35%\n",
            "Epoch 2, Batch 6000: Loss: 0.331, Accuracy: 90.35%\n",
            "Epoch 2, Batch 6100: Loss: 0.391, Accuracy: 90.33%\n",
            "Epoch 2, Batch 6200: Loss: 0.323, Accuracy: 90.34%\n",
            "Epoch 2, Batch 6300: Loss: 0.292, Accuracy: 90.35%\n",
            "Epoch 2, Batch 6400: Loss: 0.294, Accuracy: 90.36%\n",
            "Epoch 2, Batch 6500: Loss: 0.223, Accuracy: 90.40%\n",
            "Epoch 2, Batch 6600: Loss: 0.360, Accuracy: 90.40%\n",
            "Epoch 2, Batch 6700: Loss: 0.425, Accuracy: 90.38%\n",
            "Epoch 2, Batch 6800: Loss: 0.176, Accuracy: 90.43%\n",
            "Epoch 2, Batch 6900: Loss: 0.391, Accuracy: 90.40%\n",
            "Epoch 2, Batch 7000: Loss: 0.365, Accuracy: 90.41%\n",
            "Epoch 2, Batch 7100: Loss: 0.314, Accuracy: 90.43%\n",
            "Epoch 2, Batch 7200: Loss: 0.267, Accuracy: 90.47%\n",
            "Epoch 2, Batch 7300: Loss: 0.247, Accuracy: 90.50%\n",
            "Epoch 2, Batch 7400: Loss: 0.226, Accuracy: 90.56%\n",
            "Epoch 2, Batch 7500: Loss: 0.340, Accuracy: 90.57%\n",
            "Epoch 2, Batch 7600: Loss: 0.460, Accuracy: 90.54%\n",
            "Epoch 2, Batch 7700: Loss: 0.251, Accuracy: 90.56%\n",
            "Epoch 2, Batch 7800: Loss: 0.340, Accuracy: 90.57%\n",
            "Epoch 2, Batch 7900: Loss: 0.339, Accuracy: 90.55%\n",
            "Epoch 2, Batch 8000: Loss: 0.340, Accuracy: 90.55%\n",
            "Epoch 2, Batch 8100: Loss: 0.229, Accuracy: 90.59%\n",
            "Epoch 2, Batch 8200: Loss: 0.322, Accuracy: 90.61%\n",
            "Epoch 2, Batch 8300: Loss: 0.384, Accuracy: 90.59%\n",
            "Epoch 2, Batch 8400: Loss: 0.328, Accuracy: 90.60%\n",
            "Epoch 2, Batch 8500: Loss: 0.394, Accuracy: 90.60%\n",
            "Epoch 2, Batch 8600: Loss: 0.303, Accuracy: 90.60%\n",
            "Epoch 2, Batch 8700: Loss: 0.335, Accuracy: 90.60%\n",
            "Epoch 2, Batch 8800: Loss: 0.358, Accuracy: 90.60%\n",
            "Epoch 2, Batch 8900: Loss: 0.331, Accuracy: 90.61%\n",
            "Epoch 2, Batch 9000: Loss: 0.377, Accuracy: 90.61%\n",
            "Epoch 2, Batch 9100: Loss: 0.326, Accuracy: 90.62%\n",
            "Epoch 2, Batch 9200: Loss: 0.260, Accuracy: 90.62%\n",
            "Epoch 2, Batch 9300: Loss: 0.208, Accuracy: 90.65%\n",
            "Epoch 2, Batch 9400: Loss: 0.271, Accuracy: 90.68%\n",
            "Epoch 2, Batch 9500: Loss: 0.324, Accuracy: 90.69%\n",
            "Epoch 2, Batch 9600: Loss: 0.248, Accuracy: 90.72%\n",
            "Epoch 2, Batch 9700: Loss: 0.390, Accuracy: 90.73%\n",
            "Epoch 2, Batch 9800: Loss: 0.285, Accuracy: 90.72%\n",
            "Epoch 2, Batch 9900: Loss: 0.466, Accuracy: 90.70%\n",
            "Epoch 2, Batch 10000: Loss: 0.367, Accuracy: 90.72%\n",
            "Epoch 2, Batch 10100: Loss: 0.273, Accuracy: 90.74%\n",
            "Epoch 2, Batch 10200: Loss: 0.326, Accuracy: 90.74%\n",
            "Epoch 2, Batch 10300: Loss: 0.313, Accuracy: 90.73%\n",
            "Epoch 2, Batch 10400: Loss: 0.360, Accuracy: 90.74%\n",
            "Epoch 2, Batch 10500: Loss: 0.282, Accuracy: 90.74%\n",
            "Epoch 2, Batch 10600: Loss: 0.280, Accuracy: 90.76%\n",
            "Epoch 2, Batch 10700: Loss: 0.352, Accuracy: 90.77%\n",
            "Epoch 2, Batch 10800: Loss: 0.232, Accuracy: 90.80%\n",
            "Epoch 2, Batch 10900: Loss: 0.381, Accuracy: 90.79%\n",
            "Epoch 2, Batch 11000: Loss: 0.370, Accuracy: 90.80%\n",
            "Epoch 2, Batch 11100: Loss: 0.382, Accuracy: 90.78%\n",
            "Epoch 2, Batch 11200: Loss: 0.324, Accuracy: 90.79%\n",
            "Epoch 2, Batch 11300: Loss: 0.324, Accuracy: 90.79%\n",
            "Epoch 2, Batch 11400: Loss: 0.358, Accuracy: 90.78%\n",
            "Epoch 2, Batch 11500: Loss: 0.368, Accuracy: 90.76%\n",
            "Epoch 2, Batch 11600: Loss: 0.317, Accuracy: 90.76%\n",
            "Epoch 2, Batch 11700: Loss: 0.338, Accuracy: 90.76%\n",
            "Epoch 2, Batch 11800: Loss: 0.215, Accuracy: 90.79%\n",
            "Epoch 2, Batch 11900: Loss: 0.234, Accuracy: 90.82%\n",
            "Epoch 2, Batch 12000: Loss: 0.286, Accuracy: 90.83%\n",
            "Epoch 2, Batch 12100: Loss: 0.506, Accuracy: 90.81%\n",
            "Epoch 2, Batch 12200: Loss: 0.263, Accuracy: 90.82%\n",
            "Epoch 2, Batch 12300: Loss: 0.292, Accuracy: 90.83%\n",
            "Epoch 2, Batch 12400: Loss: 0.361, Accuracy: 90.82%\n",
            "Epoch 2, Batch 12500: Loss: 0.304, Accuracy: 90.82%\n",
            "Epoch 2, Batch 12600: Loss: 0.351, Accuracy: 90.81%\n",
            "Epoch 2, Batch 12700: Loss: 0.314, Accuracy: 90.81%\n",
            "Epoch 2, Batch 12800: Loss: 0.376, Accuracy: 90.80%\n",
            "Epoch 2, Batch 12900: Loss: 0.351, Accuracy: 90.79%\n",
            "Epoch 2, Batch 13000: Loss: 0.324, Accuracy: 90.81%\n",
            "Epoch 2, Batch 13100: Loss: 0.298, Accuracy: 90.81%\n",
            "Epoch 2, Batch 13200: Loss: 0.287, Accuracy: 90.82%\n",
            "Epoch 2, Batch 13300: Loss: 0.378, Accuracy: 90.81%\n",
            "Epoch 2, Batch 13400: Loss: 0.235, Accuracy: 90.83%\n",
            "Epoch 2, Batch 13500: Loss: 0.320, Accuracy: 90.82%\n",
            "Epoch 2, Batch 13600: Loss: 0.400, Accuracy: 90.81%\n",
            "Epoch 2, Batch 13700: Loss: 0.219, Accuracy: 90.83%\n",
            "Epoch 2, Batch 13800: Loss: 0.312, Accuracy: 90.83%\n",
            "Epoch 2, Batch 13900: Loss: 0.300, Accuracy: 90.84%\n",
            "Epoch 2, Batch 14000: Loss: 0.208, Accuracy: 90.87%\n",
            "Epoch 2, Batch 14100: Loss: 0.370, Accuracy: 90.87%\n",
            "Epoch 2, Batch 14200: Loss: 0.281, Accuracy: 90.86%\n",
            "Epoch 2, Batch 14300: Loss: 0.351, Accuracy: 90.87%\n",
            "Epoch 2, Batch 14400: Loss: 0.304, Accuracy: 90.87%\n",
            "Epoch 2, Batch 14500: Loss: 0.175, Accuracy: 90.89%\n",
            "Epoch 2, Batch 14600: Loss: 0.299, Accuracy: 90.90%\n",
            "Epoch 2, Batch 14700: Loss: 0.297, Accuracy: 90.90%\n",
            "Epoch 2, Batch 14800: Loss: 0.314, Accuracy: 90.90%\n",
            "Epoch 2, Batch 14900: Loss: 0.305, Accuracy: 90.90%\n",
            "Epoch 2, Batch 15000: Loss: 0.284, Accuracy: 90.91%\n",
            "Validation Accuracy after Epoch 2: 91.63%\n",
            "Saving PyTorch model...\n",
            "Model training and export completed successfully!\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Complete training script for Fashion-MNIST classification using MobileNetV2.\n",
        "This script handles data preparation, model training, and model export to multiple formats.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import gc\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.onnx\n",
        "import numpy as np\n",
        "from torchvision import models, datasets, transforms\n",
        "from PIL import Image\n",
        "import json\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "import argparse\n",
        "\n",
        "# Memory management functions\n",
        "def clear_memory():\n",
        "    \"\"\"Clear GPU and CPU memory\"\"\"\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "def setup_environment():\n",
        "    \"\"\"Configure PyTorch environment for training\"\"\"\n",
        "    # Configure PyTorch to be more memory-efficient\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    if torch.cuda.is_available():\n",
        "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:64'\n",
        "    \n",
        "    # Set up directories\n",
        "    os.makedirs('./data', exist_ok=True)\n",
        "    os.makedirs('./models', exist_ok=True)\n",
        "\n",
        "def prepare_data(batch_size=16):\n",
        "    \"\"\"Download and prepare the Fashion-MNIST dataset\"\"\"\n",
        "    print(\"Setting up data preparation...\")\n",
        "    \n",
        "    # Define data transformations\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((128, 128)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "    ])\n",
        "\n",
        "    try:\n",
        "        print(\"Downloading Fashion-MNIST dataset...\")\n",
        "        train_data = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
        "        test_data = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n",
        "        clear_memory()\n",
        "\n",
        "        # Create class mapping\n",
        "        class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "                      'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "        with open(\"index_to_name.json\", \"w\") as f:\n",
        "            json.dump({str(i): name for i, name in enumerate(class_names)}, f)\n",
        "\n",
        "        print(f\"Dataset downloaded with {len(train_data)} training and {len(test_data)} test samples.\")\n",
        "        print(f\"Class mapping created with {len(class_names)} classes.\")\n",
        "\n",
        "        # Create data loaders\n",
        "        train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "        test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "        return train_loader, test_loader, class_names\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading dataset: {e}\")\n",
        "        raise\n",
        "\n",
        "class LightweightModel(nn.Module):\n",
        "    \"\"\"MobileNetV2 model adapted for Fashion-MNIST\"\"\"\n",
        "    \n",
        "    def __init__(self, num_classes=10):\n",
        "        super(LightweightModel, self).__init__()\n",
        "        # Use MobileNetV2 pretrained on ImageNet\n",
        "        self.model = models.mobilenet_v2(pretrained=True)\n",
        "        \n",
        "        # Modify first layer to accept grayscale images (1 channel)\n",
        "        self.model.features[0][0] = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        \n",
        "        # Modify classifier to output correct number of classes\n",
        "        self.model.classifier[1] = nn.Linear(1280, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "def train_model(model, train_loader, test_loader, epochs=2, save_path=\"mobilenet_fashion_mnist.pth\"):\n",
        "    \"\"\"Train the model and evaluate on test data\"\"\"\n",
        "    \n",
        "    # Determine device (GPU or CPU)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Training on: {device}\")\n",
        "\n",
        "    # Move model to device\n",
        "    model = model.to(device)\n",
        "    \n",
        "    # Loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.7)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            \n",
        "            # Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update statistics\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            \n",
        "            if i % 50 == 49:\n",
        "                print(f'Epoch {epoch+1}, Batch {i+1}: Loss: {running_loss/50:.3f}, Accuracy: {100*correct/total:.2f}%')\n",
        "                running_loss = 0.0\n",
        "            \n",
        "            # Periodically clear memory to avoid OOM errors\n",
        "            if i % 10 == 9:\n",
        "                clear_memory()\n",
        "\n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Evaluation phase\n",
        "        model.eval()\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for images, labels in test_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                \n",
        "                # Forward pass\n",
        "                outputs = model(images)\n",
        "                \n",
        "                # Update statistics\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "        \n",
        "        # Print epoch summary\n",
        "        print(f'Validation Accuracy after Epoch {epoch+1}: {100*val_correct/val_total:.2f}%')\n",
        "        \n",
        "        # Clear memory after each epoch\n",
        "        clear_memory()\n",
        "    \n",
        "    # Save the trained model\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    print(f\"Model saved to {save_path}\")\n",
        "    \n",
        "    return model\n",
        "\n",
        "def export_for_torchserve(model, output_path=\"mobilenet_fashion_mnist.pt\"):\n",
        "    \"\"\"Export model for TorchServe\"\"\"\n",
        "    model.eval()\n",
        "    example_input = torch.randn(1, 1, 128, 128)\n",
        "    scripted_model = torch.jit.trace(model, example_input)\n",
        "    torch.jit.save(scripted_model, output_path)\n",
        "    print(f\"Model exported for TorchServe to {output_path}\")\n",
        "    return output_path\n",
        "\n",
        "def export_to_onnx(model, output_path=\"mobilenet_fashion_mnist.onnx\"):\n",
        "    \"\"\"Export model to ONNX format\"\"\"\n",
        "    model.eval()\n",
        "    example_input = torch.randn(1, 1, 128, 128)\n",
        "    \n",
        "    torch.onnx.export(\n",
        "        model,\n",
        "        example_input,\n",
        "        output_path,\n",
        "        export_params=True,\n",
        "        opset_version=12,\n",
        "        do_constant_folding=True,\n",
        "        input_names=['input'],\n",
        "        output_names=['output'],\n",
        "        dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
        "    )\n",
        "    \n",
        "    # Verify ONNX model\n",
        "    try:\n",
        "        import onnx\n",
        "        onnx_model = onnx.load(output_path)\n",
        "        onnx.checker.check_model(onnx_model)\n",
        "        print(f\"Model exported to ONNX format at {output_path} and verified successfully.\")\n",
        "        return output_path, onnx_model\n",
        "    except ImportError:\n",
        "        print(f\"Model exported to ONNX format at {output_path}, but couldn't verify (onnx package not found).\")\n",
        "        return output_path, None\n",
        "    except Exception as e:\n",
        "        print(f\"Model exported to ONNX format at {output_path}, but verification failed: {e}\")\n",
        "        return output_path, None\n",
        "\n",
        "def direct_tensorflow_export(model, class_names, output_dir=\"tensorflow_model\"):\n",
        "    \"\"\"Export to TensorFlow using Keras directly\"\"\"\n",
        "    try:\n",
        "        # Check if tensorflow is installed\n",
        "        try:\n",
        "            import tensorflow as tf\n",
        "            print(f\"TensorFlow version: {tf.__version__}\")\n",
        "        except ImportError:\n",
        "            print(\"TensorFlow not installed. Installing...\")\n",
        "            import subprocess\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"tensorflow\"])\n",
        "            import tensorflow as tf\n",
        "        \n",
        "        # Create output directory\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        saved_model_path = os.path.join(output_dir, \"saved_model\")\n",
        "        \n",
        "        print(\"Creating TensorFlow model directly...\")\n",
        "        \n",
        "        # Extract PyTorch model weights\n",
        "        model.eval()\n",
        "        state_dict = model.state_dict()\n",
        "        \n",
        "        # Create a Keras model with similar architecture to MobileNetV2\n",
        "        tf_model = tf.keras.Sequential([\n",
        "            # Input layer expecting NHWC format (TensorFlow's default)\n",
        "            tf.keras.layers.InputLayer(input_shape=(128, 128, 1)),\n",
        "            \n",
        "            # First convolutional block (similar to MobileNetV2 first layer)\n",
        "            tf.keras.layers.Conv2D(32, kernel_size=3, strides=2, padding='same', use_bias=False),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.ReLU(),\n",
        "            \n",
        "            # Several inverted residual blocks would go here in a full implementation\n",
        "            # For simplicity, we're using a more basic structure\n",
        "            tf.keras.layers.Conv2D(64, kernel_size=3, padding='same'),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.ReLU(),\n",
        "            tf.keras.layers.MaxPooling2D(pool_size=2),\n",
        "            \n",
        "            tf.keras.layers.Conv2D(128, kernel_size=3, padding='same'),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.ReLU(),\n",
        "            tf.keras.layers.MaxPooling2D(pool_size=2),\n",
        "            \n",
        "            # Global pooling and final classifier\n",
        "            tf.keras.layers.GlobalAveragePooling2D(),\n",
        "            tf.keras.layers.Dense(10)  # 10 classes for Fashion-MNIST\n",
        "        ])\n",
        "        \n",
        "        # Compile the model\n",
        "        tf_model.compile(\n",
        "            optimizer='adam',\n",
        "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "        \n",
        "        # Build model (needed to create weights)\n",
        "        tf_model.build((None, 128, 128, 1))\n",
        "        \n",
        "        # Print model summary\n",
        "        tf_model.summary()\n",
        "        \n",
        "        # Save the TensorFlow model\n",
        "        tf_model.save(saved_model_path)\n",
        "        print(f\"TensorFlow model saved to {saved_model_path}\")\n",
        "        \n",
        "        # Create a test script for the TensorFlow model\n",
        "        test_script = \"\"\"\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import json\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    # Load and preprocess image\n",
        "    image = Image.open(image_path).convert('L')\n",
        "    image = image.resize((128, 128))\n",
        "    image = np.array(image).astype(np.float32) / 255.0\n",
        "    image = (image - 0.5) / 0.5  # normalize to [-1, 1]\n",
        "    # Return in NHWC format (batch, height, width, channels)\n",
        "    return image.reshape(1, 128, 128, 1)\n",
        "\n",
        "def predict(model_path, image_path):\n",
        "    # Load model\n",
        "    model = tf.keras.models.load_model(model_path)\n",
        "    \n",
        "    # Preprocess image\n",
        "    input_tensor = preprocess_image(image_path)\n",
        "    \n",
        "    # Predict\n",
        "    prediction = model.predict(input_tensor)\n",
        "    \n",
        "    # Get class with highest probability\n",
        "    predicted_class = np.argmax(prediction, axis=1)[0]\n",
        "    \n",
        "    # Load class names\n",
        "    with open(\"index_to_name.json\", \"r\") as f:\n",
        "        class_names = json.load(f)\n",
        "    \n",
        "    print(f\"Predicted class: {{class_names[str(predicted_class)]}}\") \n",
        "    return predicted_class, class_names[str(predicted_class)]\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import sys\n",
        "    if len(sys.argv) > 1:\n",
        "        image_path = sys.argv[1]\n",
        "        model_path = \"{0}\"\n",
        "        predict(model_path, image_path)\n",
        "    else:\n",
        "        print(\"Please provide an image path to test\")\n",
        "\"\"\".format(saved_model_path)\n",
        "        \n",
        "        with open(os.path.join(output_dir, \"test_tf_model.py\"), \"w\") as f:\n",
        "            f.write(test_script)\n",
        "        \n",
        "        print(f\"Test script created at {os.path.join(output_dir, 'test_tf_model.py')}\")\n",
        "        \n",
        "        # Note about weights\n",
        "        print(\"NOTE: This TensorFlow model has a similar architecture to the PyTorch model\")\n",
        "        print(\"but does not contain the exact trained weights. It uses randomly initialized weights.\")\n",
        "        print(\"For a full conversion with matching weights, specialized conversion would be needed.\")\n",
        "        \n",
        "        return saved_model_path\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error creating TensorFlow model: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "def create_tensorflow_from_scratch(class_names, output_dir=\"tensorflow_model\"):\n",
        "    \"\"\"Create a TensorFlow model from scratch and train it directly on Fashion-MNIST\"\"\"\n",
        "    try:\n",
        "        # Check if tensorflow is installed\n",
        "        try:\n",
        "            import tensorflow as tf\n",
        "            print(f\"TensorFlow version: {tf.__version__}\")\n",
        "        except ImportError:\n",
        "            print(\"TensorFlow not installed. Installing...\")\n",
        "            import subprocess\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"tensorflow\"])\n",
        "            import tensorflow as tf\n",
        "        \n",
        "        print(\"Creating and training a TensorFlow model directly on Fashion-MNIST...\")\n",
        "        \n",
        "        # Create output directory\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        saved_model_path = os.path.join(output_dir, \"saved_model\")\n",
        "        \n",
        "        # Load Fashion-MNIST dataset directly in TensorFlow\n",
        "        fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "        (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "        \n",
        "        # Normalize and reshape images\n",
        "        train_images = train_images.astype('float32') / 255.0\n",
        "        test_images = test_images.astype('float32') / 255.0\n",
        "        \n",
        "        # Normalize to [-1, 1] to match PyTorch preprocessing\n",
        "        train_images = (train_images - 0.5) / 0.5\n",
        "        test_images = (test_images - 0.5) / 0.5\n",
        "        \n",
        "        # Reshape to include channel dimension (NHWC format)\n",
        "        train_images = train_images.reshape((-1, 28, 28, 1))\n",
        "        test_images = test_images.reshape((-1, 28, 28, 1))\n",
        "        \n",
        "        # Resize images to 128x128 to match the PyTorch model's input size\n",
        "        def resize_images(images):\n",
        "            resized = []\n",
        "            for img in images:\n",
        "                img = tf.image.resize(img, [128, 128])\n",
        "                resized.append(img)\n",
        "            return np.stack(resized)\n",
        "        \n",
        "        print(\"Resizing training images...\")\n",
        "        train_images_resized = resize_images(train_images[:5000])  # Use subset for faster training\n",
        "        print(\"Resizing test images...\")\n",
        "        test_images_resized = resize_images(test_images[:1000])    # Use subset for faster evaluation\n",
        "        \n",
        "        # Create a simple CNN model\n",
        "        model = tf.keras.Sequential([\n",
        "            # Input layer expecting NHWC format (TensorFlow's default)\n",
        "            tf.keras.layers.InputLayer(input_shape=(128, 128, 1)),\n",
        "            \n",
        "            # Convolutional layers\n",
        "            tf.keras.layers.Conv2D(32, kernel_size=3, padding='same', activation='relu'),\n",
        "            tf.keras.layers.MaxPooling2D(pool_size=2),\n",
        "            \n",
        "            tf.keras.layers.Conv2D(64, kernel_size=3, padding='same', activation='relu'),\n",
        "            tf.keras.layers.MaxPooling2D(pool_size=2),\n",
        "            \n",
        "            tf.keras.layers.Conv2D(128, kernel_size=3, padding='same', activation='relu'),\n",
        "            tf.keras.layers.MaxPooling2D(pool_size=2),\n",
        "            \n",
        "            # Flatten and dense layers\n",
        "            tf.keras.layers.Flatten(),\n",
        "            tf.keras.layers.Dense(128, activation='relu'),\n",
        "            tf.keras.layers.Dropout(0.5),\n",
        "            tf.keras.layers.Dense(10)  # 10 classes for Fashion-MNIST\n",
        "        ])\n",
        "        \n",
        "        # Compile the model\n",
        "        model.compile(\n",
        "            optimizer='adam',\n",
        "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "        \n",
        "        # Train the model\n",
        "        print(\"Training TensorFlow model...\")\n",
        "        model.fit(\n",
        "            train_images_resized, \n",
        "            train_labels[:5000],  # Use same subset as the resized images\n",
        "            epochs=2,\n",
        "            batch_size=32,\n",
        "            validation_data=(test_images_resized, test_labels[:1000])\n",
        "        )\n",
        "        \n",
        "        # Evaluate the model\n",
        "        test_loss, test_acc = model.evaluate(test_images_resized, test_labels[:1000])\n",
        "        print(f\"Test accuracy: {test_acc:.2f}\")\n",
        "        \n",
        "        # Save the model\n",
        "        model.save(saved_model_path)\n",
        "        print(f\"TensorFlow model saved to {saved_model_path}\")\n",
        "        \n",
        "        # Create a test script for the TensorFlow model\n",
        "        test_script = \"\"\"\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import json\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    # Load and preprocess image\n",
        "    image = Image.open(image_path).convert('L')\n",
        "    image = image.resize((128, 128))\n",
        "    image = np.array(image).astype(np.float32) / 255.0\n",
        "    image = (image - 0.5) / 0.5  # normalize to [-1, 1]\n",
        "    # Return in NHWC format (batch, height, width, channels)\n",
        "    return image.reshape(1, 128, 128, 1)\n",
        "\n",
        "def predict(model_path, image_path):\n",
        "    # Load model\n",
        "    model = tf.keras.models.load_model(model_path)\n",
        "    \n",
        "    # Preprocess image\n",
        "    input_tensor = preprocess_image(image_path)\n",
        "    \n",
        "    # Predict\n",
        "    prediction = model.predict(input_tensor)\n",
        "    \n",
        "    # Get class with highest probability\n",
        "    predicted_class = np.argmax(prediction, axis=1)[0]\n",
        "    \n",
        "    # Load class names\n",
        "    with open(\"index_to_name.json\", \"r\") as f:\n",
        "        class_names = json.load(f)\n",
        "    \n",
        "    print(f\"Predicted class: {{class_names[str(predicted_class)]}}\") \n",
        "    return predicted_class, class_names[str(predicted_class)]\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import sys\n",
        "    if len(sys.argv) > 1:\n",
        "        image_path = sys.argv[1]\n",
        "        model_path = \"{0}\"\n",
        "        predict(model_path, image_path)\n",
        "    else:\n",
        "        print(\"Please provide an image path to test\")\n",
        "\"\"\".format(saved_model_path)\n",
        "        \n",
        "        with open(os.path.join(output_dir, \"test_tf_model.py\"), \"w\") as f:\n",
        "            f.write(test_script)\n",
        "        \n",
        "        print(f\"Test script created at {os.path.join(output_dir, 'test_tf_model.py')}\")\n",
        "        \n",
        "        return saved_model_path\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error creating and training TensorFlow model: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"Train a MobileNetV2 model on Fashion-MNIST and export to multiple formats\")\n",
        "    parser.add_argument(\"--epochs\", type=int, default=2, help=\"Number of epochs to train\")\n",
        "    parser.add_argument(\"--batch-size\", type=int, default=16, help=\"Batch size for training\")\n",
        "    parser.add_argument(\"--output-dir\", type=str, default=\".\", help=\"Directory to save models\")\n",
        "    parser.add_argument(\"--skip-tf\", action=\"store_true\", help=\"Skip TensorFlow export\")\n",
        "    parser.add_argument(\"--tf-method\", type=str, default=\"direct\", choices=[\"direct\", \"from_scratch\"], \n",
        "                        help=\"Method for TensorFlow export: 'direct' creates a similar architecture, 'from_scratch' trains a new model\")\n",
        "    args = parser.parse_args()\n",
        "    \n",
        "    try:\n",
        "        # Setup environment\n",
        "        setup_environment()\n",
        "        \n",
        "        # Prepare data\n",
        "        train_loader, test_loader, class_names = prepare_data(batch_size=args.batch_size)\n",
        "        \n",
        "        # Create and train model\n",
        "        print(\"Initializing model...\")\n",
        "        model = LightweightModel()\n",
        "        \n",
        "        print(f\"Starting model training for {args.epochs} epochs...\")\n",
        "        trained_model = train_model(\n",
        "            model, \n",
        "            train_loader, \n",
        "            test_loader, \n",
        "            epochs=args.epochs, \n",
        "            save_path=os.path.join(args.output_dir, \"mobilenet_fashion_mnist.pth\")\n",
        "        )\n",
        "        \n",
        "        # Export model in different formats\n",
        "        print(\"Exporting model to different formats...\")\n",
        "        \n",
        "        # 1. Export for TorchServe (TorchScript format)\n",
        "        torchscript_path = export_for_torchserve(\n",
        "            trained_model, \n",
        "            output_path=os.path.join(args.output_dir, \"mobilenet_fashion_mnist.pt\")\n",
        "        )\n",
        "        \n",
        "        # 2. Export to ONNX format\n",
        "        onnx_path, _ = export_to_onnx(\n",
        "            trained_model, \n",
        "            output_path=os.path.join(args.output_dir, \"mobilenet_fashion_mnist.onnx\")\n",
        "        )\n",
        "        \n",
        "        # 3. Handle TensorFlow export\n",
        "        tf_saved_model_path = None\n",
        "        if not args.skip_tf:\n",
        "            if args.tf_method == \"direct\":\n",
        "                print(\"Creating TensorFlow model with similar architecture...\")\n",
        "                tf_saved_model_path = direct_tensorflow_export(\n",
        "                    trained_model,\n",
        "                    class_names,  # Pass class_names here\n",
        "                    output_dir=os.path.join(args.output_dir, \"tensorflow_model\")\n",
        "                )\n",
        "            else:  # from_scratch\n",
        "                print(\"Creating and training a TensorFlow model from scratch...\")\n",
        "                tf_saved_model_path = create_tensorflow_from_scratch(\n",
        "                    class_names,\n",
        "                    output_dir=os.path.join(args.output_dir, \"tensorflow_model\")\n",
        "                )\n",
        "        else:\n",
        "            print(\"Skipping TensorFlow conversion as requested.\")\n",
        "        \n",
        "        print(\"Training and export completed successfully!\")\n",
        "        print(f\"Model saved in the following formats:\")\n",
        "        print(f\"- PyTorch: {os.path.join(args.output_dir, 'mobilenet_fashion_mnist.pth')}\")\n",
        "        print(f\"- TorchScript: {torchscript_path}\")\n",
        "        print(f\"- ONNX: {onnx_path}\")\n",
        "        if tf_saved_model_path:\n",
        "            if args.tf_method == \"direct\":\n",
        "                print(f\"- TensorFlow (similar architecture): {tf_saved_model_path}\")\n",
        "            else:  # from_scratch\n",
        "                print(f\"- TensorFlow (trained from scratch): {tf_saved_model_path}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error during training or export: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
